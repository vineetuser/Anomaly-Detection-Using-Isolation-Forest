# -*- coding: utf-8 -*-
"""Anomaly Detection Model Using Isolation Forest .ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1vf3XNLJfgeDZmNOxhfabsdbTtehlauL5
"""

import os
import sys

module_path = os.path.abspath(os.path.join('../dp_common'))
if module_path not in sys.path:
    sys.path.append(module_path)

from IPython.core.interactiveshell import InteractiveShell
InteractiveShell.ast_node_interactivity = "all"

import os
from ipynb.fs.full.common_apps import get_spark_session
from ipynb.fs.full.common_apps import get_snowflake_options
from ipynb.fs.full.common_apps import get_snowflake_source

import pandas as pd
import numpy as np
import math
import matplotlib.pyplot as plt

A = '''select *, case
WHEN TXN_HR < 8 THEN '0-8'
WHEN TXN_HR >=8 AND TXN_HR <16 THEN '8-16'
WHEN TXN_HR >=16 AND TXN_HR < 24 THEN '16-24' END AS TIME_BUCKET
FROM(select a.user_id, a.USER_PROFILE_TYPE, a.transaction_time_gmt,date(a.transaction_time_gmt) as txn_date, extract(hour from a.TRANSACTION_TIME_GMT) as txn_hr,extract(day from a.TRANSACTION_TIME_GMT) as day,extract(month from a.TRANSACTION_TIME_GMT) as month,
a.CARD_PROGRAM, a.REQUESTED_AMOUNT, a.REQUESTED_AMOUNT_CURRENCY, a.TRANSACTION_AMOUNT,
(cast((b.credit_limit - a.AVAILABLE_BALANCE) as float)/cast(b.CREDIT_LIMIT as float))*100 as cc_utilised_prcnt,
a.mcc, a.PAN_ENTRY_MODE, a.TRANSACTION_ID, a.ORIGINAL_TRANSACTION_ID, b.USER_BUREAU_STATUS, b.CREDIT_LIMIT,
(date(getdate()) - date(b.CREDIT_CARD_CREATED_TIME)) as cc_age
from zolve.facts.OVERALL_TRANSACTIONS a
left join zolve.facts.USER_ONBOARDING_STAGES b
on a.user_id = b.user_id
where a.transaction_status = 'SUCCESS' and a.PRODUCT_TYPE = 'Credit Card' and a.IS_INTERNAL_TRANSACTION = 'no' and b.user_category <> 'fnf' and a.TRANSACTION_AMOUNT <> 0
and a.TRANSACTION_SUB_STATUS <> 'reversal' and a.transaction_type = 'debit' AND a.TRANSACTION_TYPE_CODE in ('00'))
'''

spark = get_spark_session("Anomaly Detection", swan_spark_conf)

# Enable Arrow-based columnar data transfers
spark.conf.set("spark.sql.execution.arrow.enabled", "true")

try:
    data_frame = spark.read.format(get_snowflake_source()) \
    .options(**get_snowflake_options()) \
    .option("query", A) \
    .load()
    df = data_frame.toPandas()
except Exception as exp:
    print(exp)
spark.stop()
df.head()

df1 = df.drop_duplicates()

import datetime as dt
df1['TRANSACTION_TIME_GMT'] = pd.to_datetime(df1['TRANSACTION_TIME_GMT'])
df1['TXN_TIME'] = df1['TRANSACTION_TIME_GMT'].dt.time

df2 = df1.groupby(['USER_ID','TRANSACTION_ID'])['TRANSACTION_ID'].count().reset_index(name = 'COUNT')

df3 = df1.merge(df2, on = ['USER_ID','TRANSACTION_ID'])

df3 = df3.sort_values(by = ['USER_ID', 'DAY', 'MONTH', 'TXN_TIME'])

df3['COUNT_24HR'] = df3.groupby(['USER_ID', 'DAY','MONTH'])['COUNT'].cumsum(axis=0)

df3['AMT_24HR'] = df3.groupby(['USER_ID', 'DAY', 'MONTH'])['TRANSACTION_AMOUNT'].cumsum(axis=0)

df3['COUNT_TIME_BUCKET'] = df3.groupby(['USER_ID', 'DAY', 'MONTH','TIME_BUCKET'])['COUNT'].cumsum(axis=0)

df3['WEEK_DAY']  = df3['TRANSACTION_TIME_GMT'].dt.day_name()

# df3.drop(['TRANSACTION_TIME_GMT','COUNT','TXN_TIME','MONTH','DAY'],axis=1, inplace=True)

df3['TOTAL_COUNTS'] = df3.groupby(['USER_ID'])['COUNT'].cumsum(axis=0)

df3['TOTAL_AMOUNT'] = df3.groupby(['USER_ID'])['TRANSACTION_AMOUNT'].cumsum(axis=0)



df4 = df3.groupby(['USER_ID','TXN_DATE'])['USER_ID'].count()

df4

df3[df3['USER_ID']=='0002be66-d809-4ca1-a5a0-4a522adc9323']

df3.head()

from sklearn.preprocessing import LabelEncoder
le = LabelEncoder()
df3['CARD_PROGRAM'] = le.fit_transform(df3['CARD_PROGRAM'])
df3['USER_PROFILE_TYPE'] = le.fit_transform(df3['USER_PROFILE_TYPE'])
df3['REQUESTED_AMOUNT_CURRENCY'] = le.fit_transform(df3['REQUESTED_AMOUNT_CURRENCY'])
df3['USER_BUREAU_STATUS'] = le.fit_transform(df3['USER_BUREAU_STATUS'])
df3['TIME_BUCKET'] = le.fit_transform(df3['TIME_BUCKET'])
df3['WEEK_DAY'] = le.fit_transform(df3['WEEK_DAY'])

df3['PAN_ENTRY_MODE'] = df3['PAN_ENTRY_MODE'].replace('***', '000')

df3.info()

df3['TXN_HR'] = df3['TXN_HR'].astype(int)
df3['MCC'] = df3['MCC'].astype(int)
df3['PAN_ENTRY_MODE'] = df3['PAN_ENTRY_MODE'].astype(int)
df3['CC_AGE'] = df3['CC_AGE'].astype(int)

features = pd.DataFrame(index=df3.index)

numerical_columns = ['USER_PROFILE_TYPE', 'TXN_HR', 'CARD_PROGRAM','REQUESTED_AMOUNT', 'REQUESTED_AMOUNT_CURRENCY',
       'TRANSACTION_AMOUNT','CC_UTILISED_PRCNT', 'MCC', 'PAN_ENTRY_MODE','USER_BUREAU_STATUS', 'CREDIT_LIMIT',
       'CC_AGE', 'TIME_BUCKET', 'COUNT_24HR', 'AMT_24HR', 'COUNT_TIME_BUCKET','WEEK_DAY']
features[numerical_columns] = df3[numerical_columns]

features.head()

features.dropna(how='any', inplace=True)
features.info()

# to check rows if any null values are there
# features[features.isna().any(axis=1)]

from sklearn.ensemble import IsolationForest
forest = IsolationForest(random_state= 1, n_estimators= 100, contamination = float(0.01), max_samples= 'auto')
forest.fit(features)

scores = forest.score_samples(features)

df3.dropna(subset = ['CC_UTILISED_PRCNT'], inplace=True)

df3['anamoly_scores'] = scores

df3['anomaly'] = forest.predict(features)
df3.head()

df3['anomaly'].value_counts()

df3.head()

df3['today'] = dt.date.today()

df3['today'] = pd.to_datetime(df3['today'])
df3['TXN_DATE'] = pd.to_datetime(df3['TXN_DATE'])
df3['day_diff'] = df3['today'] - df3['TXN_DATE']
last_day_txn = df3[df3['day_diff']== '1 days']
df3.drop(['day_diff','today'], axis=1, inplace=True)
last_day_txn.drop(['day_diff','today'], axis=1, inplace=True)

len(last_day_txn)

anomaly_data = last_day_txn[last_day_txn['anomaly']==-1]
anomaly_data.head()

len(anomaly_data)

len(anomaly_data['USER_ID'].unique())

org_txn_id = anomaly_data['ORIGINAL_TRANSACTION_ID'].unique()